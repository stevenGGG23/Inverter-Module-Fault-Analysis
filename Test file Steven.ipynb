{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b5c7396-c7e4-4b67-bb4e-7c315c543ee1",
   "metadata": {},
   "source": [
    "### Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39fc63a-e9e7-446a-9ff2-e8a70121d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from datetime import datetime, timedelta, timezone\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11c280b-8c8c-4702-b205-7d17bd68c6bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CSV File Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c71044-85ed-496f-8d15-dc47599c7561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_site_configuration(csv_file_path): \n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        sites_df = pd.read_csv(csv_file_path)\n",
    "        \n",
    "        # Filter out rows where Project Name is empty/null\n",
    "        sites_df = sites_df.dropna(subset=['Project Name'])\n",
    "        sites_df = sites_df[sites_df['Project Name'].str.strip() != '']\n",
    "        \n",
    "        # Get unique site names\n",
    "        site_names = sites_df['Project Name'].unique().tolist()\n",
    "        \n",
    "        print(f\"Loaded {len(site_names)} sites from configuration file:\")\n",
    "        for i, site in enumerate(site_names, 1):\n",
    "            print(f\"  {i}. {site}\")\n",
    "        \n",
    "        return site_names, sites_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading site configuration: {e}\")\n",
    "        return [], pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f1eb39-0daa-40d9-96af-9f0c79dd5c8b",
   "metadata": {},
   "source": [
    "### Generate metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa1691c-f51a-49ee-9ce6-8ccd9914c90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate metadata for inverter module state signals for a specific site\n",
    "def generate_inv_module_metadata(site_name):\n",
    "    try:\n",
    "        print(f\"Generating metadata for site: {site_name}\")\n",
    "        \n",
    "        # Search for inverter module signals\n",
    "        inv_mod_ac_power = spy.search({\n",
    "            'Path': f'GPM >> {site_name} >> Inverter module',\n",
    "            'Name': 'Active Power',\n",
    "            'Type': 'Signal'\n",
    "        }).sort_values('Asset').reset_index(drop=True)\n",
    "        \n",
    "        inv_mod_ac_power = inv_mod_ac_power[\n",
    "            (inv_mod_ac_power[\"Name\"] == 'ACTIVE POWER') & \n",
    "            (inv_mod_ac_power[\"Asset\"].str.contains(\"Module\"))\n",
    "        ].reset_index(drop=True)\n",
    "        \n",
    "        # look for POWERELECTRONICS HEx CURRENT FAULT\n",
    "        inv_mod_fault_code = spy.search({\n",
    "            'Path': f'GPM >> {site_name} >> Inverter module',\n",
    "            'Name': 'POWERELECTRONICS HEx CURRENT FAULT',\n",
    "            'Type': 'Signal'\n",
    "        }).sort_values(['Asset']).reset_index(drop=True)\n",
    "        \n",
    "        inv_mod_comms_status = spy.search({\n",
    "            'Path': f'GPM >> {site_name} >> Inverter module',\n",
    "            'Name': 'COMS STATUS',\n",
    "            'Type': 'Signal'\n",
    "        }).sort_values('Asset').reset_index(drop=True)\n",
    "        \n",
    "        plant_irrad = spy.search({\n",
    "            'Path': f'GPM >> {site_name} >> Customized element',\n",
    "            'Name': 'Average Filtered POA_sub (HC)',\n",
    "            'Type': 'Signal'\n",
    "        })\n",
    "        \n",
    "        if plant_irrad.empty:\n",
    "            print(f\"Warning: No irradiance data found for {site_name}\")\n",
    "            return None\n",
    "        \n",
    "        # Generate metadata for each module\n",
    "        metadata_list = []\n",
    "        for i in range(len(inv_mod_ac_power.index)):\n",
    "            formula = '''$no_data = $ap.isNotValid()\n",
    "            $sufficient_irrad = ($irrad>20).merge(0min).removeShorterThan(20min)\n",
    "            $inv_no_power = ($ap==0).merge(0min).removeShorterThan(20min)\n",
    "            $inv_online = ($ap>0).intersect($sufficient_irrad)\n",
    "            $comms_out = ($comstat == 255).merge(0min).removeShorterThan(20min)\n",
    "            $fault_active = ($faultcode !=0).merge(0min).removeShorterThan(20min)\n",
    "            $inv_good_comms = ($comstat == 0).merge(0min).removeShorterThan(20min)\n",
    "            $fault_inactive = ($faultcode ==0).merge(0min).removeShorterThan(20min)\n",
    "            $der = $ap.derivative('h')\n",
    "            $freeze = ($der == 0)\n",
    "            $comms_freeze = $freeze.intersect($sufficient_irrad).intersect($inv_online).intersect($fault_inactive).removeShorterThan(20min)\n",
    "            $inv_offline = ($inv_no_power).intersect($sufficient_irrad)\n",
    "            $inv_comms_out = ($inv_offline).intersect($comms_out).removeShorterThan(20min)\n",
    "            $inv_off_fault = $inv_offline.intersect($inv_good_comms).intersect($fault_active).removeShorterThan(20min)\n",
    "            $inv_off_no_fault = $inv_offline.intersect($inv_good_comms).intersect($fault_inactive).removeShorterThan(20min)\n",
    "            \"0\".splice(\"OFFLINE WITH FAULT\",$inv_off_fault).splice(\"OFFLINE WITHOUT FAULT\",$inv_off_no_fault).splice(\"COMMS OUTAGE\",$inv_comms_out).splice(\"COMMS FREEZE\",$comms_freeze).splice(\"MISSING DATA\",$no_data)'''\n",
    "            \n",
    "            metadata_df = {\n",
    "                'Path': inv_mod_ac_power['Path'].iloc[i],\n",
    "                'Asset': inv_mod_ac_power['Asset'].iloc[i],\n",
    "                'Name': 'Inv Module State',\n",
    "                'Type': 'Signal',              \n",
    "                'Formula': formula,\n",
    "                'Formula Parameters': {\n",
    "                    '$irrad': plant_irrad.loc[0,'ID'],\n",
    "                    '$ap': inv_mod_ac_power.loc[i,'ID'],\n",
    "                    '$comstat': inv_mod_comms_status.loc[i,'ID'],\n",
    "                    '$faultcode': inv_mod_fault_code.loc[i,'ID']\n",
    "                }\n",
    "            }\n",
    "            metadata_list.append(pd.DataFrame([metadata_df]))\n",
    "        \n",
    "        all_metadata = pd.concat(metadata_list, ignore_index=True)\n",
    "        print(f\"Generated metadata for {len(all_metadata)} modules at {site_name}\")\n",
    "        \n",
    "        return all_metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating metadata for {site_name}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ee7a2b-982d-471a-a161-241ab3628f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze fault codes and return fault periods with start/end dates for each module\n",
    "def analyze_fault_periods(site_name, filtered_data, fault_codes):\n",
    "\n",
    "    if fault_codes is None or (filtered_data is None or filtered_data.empty):\n",
    "        print(f\"No fault code data available for analysis at {site_name}\")\n",
    "        return []\n",
    "    \n",
    "    fault_periods = []\n",
    "    \n",
    "    # Analyze each module\n",
    "    for column in fault_codes.columns:\n",
    "        module_faults = fault_codes[column].dropna()\n",
    "        \n",
    "        if module_faults.empty:\n",
    "            continue\n",
    "            \n",
    "        # Find fault periods (non-zero fault codes)\n",
    "        fault_active = module_faults[module_faults != 0]\n",
    "        \n",
    "        if fault_active.empty:\n",
    "            continue\n",
    "            \n",
    "        # Group consecutive fault periods\n",
    "        current_fault = None\n",
    "        start_time = None\n",
    "        prev_time = None\n",
    "        \n",
    "        for timestamp, fault_code in fault_active.items():\n",
    "            if current_fault is None:\n",
    "                # Start of a new fault period\n",
    "                current_fault = fault_code\n",
    "                start_time = timestamp\n",
    "                prev_time = timestamp\n",
    "            elif fault_code != current_fault or (timestamp - prev_time).total_seconds() > 1800:  # 30 min gap\n",
    "                # Fault code changed or significant time gap, end previous period\n",
    "                fault_periods.append({\n",
    "                    'Site': site_name,\n",
    "                    'Module': column,\n",
    "                    'Start': start_time,\n",
    "                    'End': prev_time,\n",
    "                    'Fault_Code': current_fault,\n",
    "                    'Duration_Hours': round((prev_time - start_time).total_seconds() / 3600, 2)\n",
    "                })\n",
    "                current_fault = fault_code\n",
    "                start_time = timestamp\n",
    "            \n",
    "            prev_time = timestamp\n",
    "        \n",
    "        # Don't forget the last fault period\n",
    "        if current_fault is not None:\n",
    "            fault_periods.append({\n",
    "                'Site': site_name,\n",
    "                'Module': column,\n",
    "                'Start': start_time,\n",
    "                'End': prev_time,\n",
    "                'Fault_Code': current_fault,\n",
    "                'Duration_Hours': round((prev_time - start_time).total_seconds() / 3600, 2)\n",
    "            })\n",
    "    \n",
    "    return fault_periods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480d649f-b091-4fba-8918-dc6740eb52ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run fault analysis for all sites and generate comprehensive report\n",
    "def run_multi_site_analysis(csv_file_path, start_date, end_date, output_file='fault_analysis_report.csv', grid='5min'):\n",
    " \n",
    "    print(\"=\"*80)\n",
    "    print(\"MULTI-SITE INVERTER MODULE FAULT ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load site configuration\n",
    "    site_names, sites_df = load_site_configuration(csv_file_path)\n",
    "    \n",
    "    if not site_names:\n",
    "        print(\"No sites found in configuration file\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize results storage\n",
    "    all_fault_periods = []\n",
    "    site_summary = []\n",
    "    \n",
    "    # Process each site\n",
    "    for site_name in site_names:\n",
    "        try:\n",
    "            fault_periods = analyze_single_site(site_name, start_date, end_date, grid) \n",
    "             \n",
    "            if fault_periods:\n",
    "                all_fault_periods.extend(fault_periods)\n",
    "                \n",
    "                # Create site summary\n",
    "                site_summary.append({\n",
    "                    'Site': site_name,\n",
    "                    'Total_Fault_Periods': len(fault_periods),\n",
    "                    'Total_Modules_Affected': len(set([fp['Module'] for fp in fault_periods])),\n",
    "                    'Total_Fault_Hours': sum([fp['Duration_Hours'] for fp in fault_periods]),\n",
    "                    'Analysis_Date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                })\n",
    "            else:\n",
    "                site_summary.append({\n",
    "                    'Site': site_name,\n",
    "                    'Total_Fault_Periods': 0,\n",
    "                    'Total_Modules_Affected': 0,\n",
    "                    'Total_Fault_Hours': 0,\n",
    "                    'Analysis_Date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to analyze {site_name}: {e}\")\n",
    "            site_summary.append({\n",
    "                'Site': site_name,\n",
    "                'Total_Fault_Periods': 'ERROR',\n",
    "                'Total_Modules_Affected': 'ERROR',\n",
    "                'Total_Fault_Hours': 'ERROR',\n",
    "                'Analysis_Date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            })\n",
    "    \n",
    "    # Convert results to DataFrames\n",
    "    if all_fault_periods:\n",
    "        fault_df = pd.DataFrame(all_fault_periods)\n",
    "        # Format datetime columns\n",
    "        fault_df['Start'] = pd.to_datetime(fault_df['Start'])\n",
    "        fault_df['End'] = pd.to_datetime(fault_df['End'])\n",
    "        fault_df['Start_Formatted'] = fault_df['Start'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        fault_df['End_Formatted'] = fault_df['End'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # Reorder columns for better readability\n",
    "        fault_df = fault_df[['Site', 'Module', 'Start_Formatted', 'End_Formatted', \n",
    "                            'Fault_Code', 'Duration_Hours']]\n",
    "    else:\n",
    "        fault_df = pd.DataFrame()\n",
    "    \n",
    "    summary_df = pd.DataFrame(site_summary)\n",
    "    \n",
    "    # Save results to CSV\n",
    "    try:\n",
    "        if not fault_df.empty:\n",
    "            fault_df.to_csv(output_file, index=False)\n",
    "            print(f\"\\nFault analysis report saved to: {output_file}\")\n",
    "        \n",
    "        summary_file = output_file.replace('.csv', '_summary.csv')\n",
    "        summary_df.to_csv(summary_file, index=False)\n",
    "        print(f\"Site summary saved to: {summary_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results: {e}\")\n",
    "    \n",
    "    # Display summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    total_sites = len(site_names)\n",
    "    sites_with_faults = len([s for s in site_summary if s['Total_Fault_Periods'] > 0])\n",
    "    total_fault_periods = sum([s['Total_Fault_Periods'] for s in site_summary if isinstance(s['Total_Fault_Periods'], int)])\n",
    "    \n",
    "    print(f\"Total Sites Analyzed: {total_sites}\")\n",
    "    print(f\"Sites with Faults: {sites_with_faults}\")\n",
    "    print(f\"Total Fault Periods: {total_fault_periods}\")\n",
    "    \n",
    "    if not fault_df.empty:\n",
    "        print(f\"Total Fault Hours: {fault_df['Duration_Hours'].sum():.2f}\")\n",
    "        print(f\"Average Fault Duration: {fault_df['Duration_Hours'].mean():.2f} hours\")\n",
    "    \n",
    "    return fault_df, summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0a2dd3-cb8c-41a9-a54a-8fa3f9562291",
   "metadata": {},
   "source": [
    "### Data Filtering Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e65e224-7fcc-4eff-aef4-c40f56a576cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ims_data(site_name, start_date, end_date, grid='5min', target_states=None):\n",
    "    try:\n",
    "        # Default target states\n",
    "        if target_states is None:\n",
    "            target_states = ['OFFLINE WITH FAULT', 'OFFLINE W/O FAULT', 'COMMS OUTAGE']\n",
    "        \n",
    "        # Search for Inv Module State signals\n",
    "        mod_state = spy.search({\n",
    "            'Path': f'GPM >> {site_name}',\n",
    "            'Name': 'Inv Module State',\n",
    "            'Type': 'Signal'\n",
    "        }).sort_values('Asset').reset_index(drop=True)\n",
    "        \n",
    "        if mod_state.empty:\n",
    "            print(f\"No Inv Module State signals found for {site_name}\")\n",
    "            return None, None\n",
    "        \n",
    "        # Pull the data for the found signals\n",
    "        mod_data = spy.pull(\n",
    "            mod_state, \n",
    "            start=start_date, \n",
    "            end=end_date, \n",
    "            grid=grid\n",
    "        )\n",
    "        \n",
    "        # Filter the data to isolate modules with target states\n",
    "        filtered_data = mod_data[mod_data.isin(target_states)]\n",
    "        \n",
    "        # Remove rows where all values are NaN (no target states found)\n",
    "        filtered_data = filtered_data.dropna(how='all')\n",
    "        \n",
    "        # Get fault codes for offline modules\n",
    "        fault_codes = None\n",
    "        try:\n",
    "            # Search for fault code signals\n",
    "            fault_code_signals = spy.search({\n",
    "                'Path': f'GPM >> {site_name} >> Inverter module',\n",
    "                'Name': 'POWERELECTRONICS HEx CURRENT FAULT',\n",
    "                'Type': 'Signal'\n",
    "            }).sort_values(['Asset']).reset_index(drop=True)\n",
    "            \n",
    "            if not fault_code_signals.empty:\n",
    "                # Pull fault code data\n",
    "                fault_codes = spy.pull(\n",
    "                    fault_code_signals,\n",
    "                    start=start_date,\n",
    "                    end=end_date,\n",
    "                    grid=grid\n",
    "                )\n",
    "                \n",
    "                print(f\"Found fault codes for {len(fault_code_signals)} modules at {site_name}\")\n",
    "            else:\n",
    "                print(f\"No fault code signals found for {site_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Could not retrieve fault codes for {site_name}: {e}\")\n",
    "            fault_codes = None\n",
    "        \n",
    "        return filtered_data, fault_codes\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error filtering data for {site_name}: {e}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05c55e5-ae0d-416f-af59-17a2328903b2",
   "metadata": {},
   "source": [
    "### Fault Period Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab78de42-a52b-4211-96fe-b222ab02f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze fault codes and return fault periods with start/end dates for each module\n",
    "def analyze_fault_periods(site_name, filtered_data, fault_codes):\n",
    "    \n",
    "    if fault_codes is None or (filtered_data is None or filtered_data.empty):\n",
    "        print(f\"No fault code data available for analysis at {site_name}\")\n",
    "        return []\n",
    "    \n",
    "    fault_periods = []\n",
    "    \n",
    "    # Analyze each module\n",
    "    for column in fault_codes.columns:\n",
    "        module_faults = fault_codes[column].dropna()\n",
    "        \n",
    "        if module_faults.empty:\n",
    "            continue\n",
    "            \n",
    "        # Find fault periods (non-zero fault codes)\n",
    "        fault_active = module_faults[module_faults != 0]\n",
    "        \n",
    "        if fault_active.empty:\n",
    "            continue\n",
    "            \n",
    "        # Group consecutive fault periods\n",
    "        current_fault = None\n",
    "        start_time = None\n",
    "        prev_time = None\n",
    "        \n",
    "        for timestamp, fault_code in fault_active.items():\n",
    "            if current_fault is None:\n",
    "                # Start of a new fault period\n",
    "                current_fault = fault_code\n",
    "                start_time = timestamp\n",
    "                prev_time = timestamp\n",
    "            elif fault_code != current_fault or (timestamp - prev_time).total_seconds() > 1800:  # 30 min gap\n",
    "                # Fault code changed or significant time gap, end previous period\n",
    "                fault_periods.append({\n",
    "                    'Site': site_name,\n",
    "                    'Module': column,\n",
    "                    'Start': start_time,\n",
    "                    'End': prev_time,\n",
    "                    'Fault_Code': current_fault,\n",
    "                    'Duration_Hours': round((prev_time - start_time).total_seconds() / 3600, 2)\n",
    "                })\n",
    "                current_fault = fault_code\n",
    "                start_time = timestamp\n",
    "            \n",
    "            prev_time = timestamp\n",
    "        \n",
    "        # Don't forget the last fault period\n",
    "        if current_fault is not None:\n",
    "            fault_periods.append({\n",
    "                'Site': site_name,\n",
    "                'Module': column,\n",
    "                'Start': start_time,\n",
    "                'End': prev_time,\n",
    "                'Fault_Code': current_fault,\n",
    "                'Duration_Hours': round((prev_time - start_time).total_seconds() / 3600, 2)\n",
    "            })\n",
    "    \n",
    "    return fault_periods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c250a3-ec3f-4b99-9234-8d2e94e21798",
   "metadata": {},
   "source": [
    "### Single Site analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5334ae62-9137-42e7-84d1-dab393f032fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_single_site(site_name, start_date, end_date, grid='5min'):\n",
    "\n",
    "    print(f\"Analyzing {site_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Filter data\n",
    "        filtered_data, fault_codes = filter_ims_data(site_name, start_date, end_date, grid)\n",
    "        \n",
    "        if fault_codes is not None:\n",
    "            # Analyze fault periods\n",
    "            fault_periods = analyze_fault_periods(site_name, filtered_data, fault_codes)\n",
    "            \n",
    "            if fault_periods:\n",
    "                print(f\"  Found {len(fault_periods)} fault periods\")\n",
    "                return fault_periods\n",
    "            else:\n",
    "                print(f\"  No fault periods found\")\n",
    "                return []\n",
    "        else:\n",
    "            print(f\"  Could not retrieve fault code data\")\n",
    "            return []\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  Error analyzing {site_name}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1317f237-b113-4528-9632-ea0a699bd70d",
   "metadata": {},
   "source": [
    "### All site analysis and report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f067209-1d9b-44c0-bc33-eaf40024f4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multi_site_analysis(csv_file_path, start_date, end_date, output_file='fault_analysis_report.csv', grid='5min'):\n",
    "    print(\"Starting Multi-Site Inverter Module Fault Analysis...\")\n",
    "    \n",
    "    # Load site configuration\n",
    "    site_names, sites_df = load_site_configuration(csv_file_path)\n",
    "    \n",
    "    if not site_names:\n",
    "        print(\"No sites found in configuration file\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize results storage\n",
    "    all_fault_periods = []\n",
    "    site_summary = []\n",
    "    \n",
    "    # Process each site\n",
    "    for site_name in site_names:\n",
    "        try:\n",
    "            fault_periods = analyze_single_site(site_name, start_date, end_date, grid)\n",
    "            \n",
    "            if fault_periods:\n",
    "                all_fault_periods.extend(fault_periods)\n",
    "                \n",
    "                # Create site summary\n",
    "                site_summary.append({\n",
    "                    'Site': site_name,\n",
    "                    'Total_Fault_Periods': len(fault_periods),\n",
    "                    'Total_Modules_Affected': len(set([fp['Module'] for fp in fault_periods])),\n",
    "                    'Total_Fault_Hours': sum([fp['Duration_Hours'] for fp in fault_periods]),\n",
    "                    'Analysis_Date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                })\n",
    "            else:\n",
    "                site_summary.append({\n",
    "                    'Site': site_name,\n",
    "                    'Total_Fault_Periods': 0,\n",
    "                    'Total_Modules_Affected': 0,\n",
    "                    'Total_Fault_Hours': 0,\n",
    "                    'Analysis_Date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Failed to analyze {site_name}: {e}\")\n",
    "            site_summary.append({\n",
    "                'Site': site_name,\n",
    "                'Total_Fault_Periods': 'ERROR',\n",
    "                'Total_Modules_Affected': 'ERROR',\n",
    "                'Total_Fault_Hours': 'ERROR',\n",
    "                'Analysis_Date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            })\n",
    "    \n",
    "    # Convert results to DataFrames\n",
    "    if all_fault_periods:\n",
    "        fault_df = pd.DataFrame(all_fault_periods)\n",
    "        # Format datetime columns\n",
    "        fault_df['Start'] = pd.to_datetime(fault_df['Start'])\n",
    "        fault_df['End'] = pd.to_datetime(fault_df['End'])\n",
    "        fault_df['Start_Formatted'] = fault_df['Start'].dt.strftime('%m/%d/%Y %I %p')\n",
    "        fault_df['End_Formatted'] = fault_df['End'].dt.strftime('%m/%d/%Y %I %p')\n",
    "        \n",
    "        # Create the formatted fault period string\n",
    "        fault_df['Fault_Period'] = (fault_df['Module'] + ': ' + \n",
    "                                   fault_df['Start_Formatted'] + ' - ' + \n",
    "                                   fault_df['End_Formatted'] + ', Fault=' + \n",
    "                                   fault_df['Fault_Code'].astype(str))\n",
    "        \n",
    "        # Reorder columns for better readability\n",
    "        fault_df = fault_df[['Site', 'Module', 'Start_Formatted', 'End_Formatted', \n",
    "                            'Fault_Code', 'Duration_Hours', 'Fault_Period']]\n",
    "    else:\n",
    "        fault_df = pd.DataFrame()\n",
    "    \n",
    "    summary_df = pd.DataFrame(site_summary)\n",
    "    \n",
    "    # Save results to CSV\n",
    "    try:\n",
    "        if not fault_df.empty:\n",
    "            fault_df.to_csv(output_file, index=False)\n",
    "            print(f\"Fault analysis report saved to: {output_file}\")\n",
    "        \n",
    "        summary_file = output_file.replace('.csv', '_summary.csv')\n",
    "        summary_df.to_csv(summary_file, index=False)\n",
    "        print(f\"Site summary saved to: {summary_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results: {e}\")\n",
    "    \n",
    "    # Display summary\n",
    "    print(\"\\nAnalysis Summary:\")\n",
    "    \n",
    "    total_sites = len(site_names)\n",
    "    sites_with_faults = len([s for s in site_summary if s['Total_Fault_Periods'] > 0])\n",
    "    total_fault_periods = sum([s['Total_Fault_Periods'] for s in site_summary if isinstance(s['Total_Fault_Periods'], int)])\n",
    "    \n",
    "    print(f\"Total Sites Analyzed: {total_sites}\")\n",
    "    print(f\"Sites with Faults: {sites_with_faults}\")\n",
    "    print(f\"Total Fault Periods: {total_fault_periods}\")\n",
    "    \n",
    "    if not fault_df.empty:\n",
    "        print(f\"Total Fault Hours: {fault_df['Duration_Hours'].sum():.2f}\")\n",
    "        print(f\"Average Fault Duration: {fault_df['Duration_Hours'].mean():.2f} hours\")\n",
    "    \n",
    "    return fault_df, summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff850e63-8a6b-4d75-bfef-bf7f0ba36b74",
   "metadata": {},
   "source": [
    "### Run main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73824eb6-7179-4498-8e50-e315bf6ec9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Multi-Site Inverter Module Fault Analysis...\n",
      "Date Range: 2025-07-07 to 2025-07-09\n",
      "Grid Resolution: 5min\n",
      "Output File: inverter_fault_analysis_report.csv\n",
      "Starting Multi-Site Inverter Module Fault Analysis...\n",
      "Loaded 21 sites from configuration file:\n",
      "  1. Aerojet- Camden\n",
      "  2. Carey\n",
      "  3. Carey B\n",
      "  4. Fort Lupton\n",
      "  5. Haywood\n",
      "  6. Houston\n",
      "  7. Kersey\n",
      "  8. Mavericks\n",
      "  9. Millington\n",
      "  10. New Albany A\n",
      "  11. New Albany B\n",
      "  12. Platte\n",
      "  13. Providence\n",
      "  14. Ripley (Energize)\n",
      "  15. Ripley (PS Lauderdale)\n",
      "  16. Ripley (SEPI)\n",
      "  17. Ripley (SR)\n",
      "  18. Selmer 1\n",
      "  19. Selmer 2\n",
      "  20. South Loving\n",
      "  21. Windsor\n",
      "Analyzing Aerojet- Camden...\n",
      "Error filtering data for Aerojet- Camden: name 'spy' is not defined\n",
      "  Could not retrieve fault code data\n",
      "Analyzing Carey...\n",
      "Error filtering data for Carey: name 'spy' is not defined\n",
      "  Could not retrieve fault code data\n",
      "Analyzing Carey B...\n",
      "Error filtering data for Carey B: name 'spy' is not defined\n",
      "  Could not retrieve fault code data\n",
      "Analyzing Fort Lupton...\n",
      "Error filtering data for Fort Lupton: name 'spy' is not defined\n",
      "  Could not retrieve fault code data\n",
      "Analyzing Haywood...\n",
      "Error filtering data for Haywood: name 'spy' is not defined\n",
      "  Could not retrieve fault code data\n",
      "Analyzing Houston...\n",
      "Error filtering data for Houston: name 'spy' is not defined\n",
      "  Could not retrieve fault code data\n",
      "Analyzing Kersey...\n",
      "Error filtering data for Kersey: name 'spy' is not defined\n",
      "  Could not retrieve fault code data\n",
      "Analyzing Mavericks...\n",
      "Error filtering data for Mavericks: name 'spy' is not defined\n",
      "  Could not retrieve fault code data\n",
      "Analyzing Millington...\n",
      "Error filtering data for Millington: name 'spy' is not defined\n",
      "  Could not retrieve fault code data\n",
      "Analyzing New Albany A...\n",
      "Error filtering data for New Albany A: name 'spy' is not defined\n",
      "  Could not retrieve fault code data\n",
      "Analyzing New Albany B...\n",
      "Error filtering data for New Albany B: name 'spy' is not defined\n",
      "  Could not retrieve fault code data\n",
      "Analyzing Platte...\n",
      "Error filtering data for Platte: name 'spy' is not defined\n",
      "  Could not retrieve fault code data\n",
      "Analyzing Providence...\n",
      "Error filtering data for Providence: name 'spy' is not defined\n",
      "  Could not retrieve fault code data\n",
      "Analyzing Ripley (Energize)...\n",
      "Error filtering data for Ripley (Energize): name 'spy' is not defined\n",
      "  Could not retrieve fault code data\n",
      "Analyzing Ripley (PS Lauderdale)...\n",
      "Error filtering data for Ripley (PS Lauderdale): name 'spy' is not defined\n",
      "  Could not retrieve fault code data\n",
      "Analyzing Ripley (SEPI)...\n",
      "Error filtering data for Ripley (SEPI): name 'spy' is not defined\n",
      "  Could not retrieve fault code data\n",
      "Analyzing Ripley (SR)...\n",
      "Error filtering data for Ripley (SR): name 'spy' is not defined\n",
      "  Could not retrieve fault code data\n",
      "Analyzing Selmer 1...\n",
      "Error filtering data for Selmer 1: name 'spy' is not defined\n",
      "  Could not retrieve fault code data\n",
      "Analyzing Selmer 2...\n",
      "Error filtering data for Selmer 2: name 'spy' is not defined\n",
      "  Could not retrieve fault code data\n",
      "Analyzing South Loving...\n",
      "Error filtering data for South Loving: name 'spy' is not defined\n",
      "  Could not retrieve fault code data\n",
      "Analyzing Windsor...\n",
      "Error filtering data for Windsor: name 'spy' is not defined\n",
      "  Could not retrieve fault code data\n",
      "Site summary saved to: inverter_fault_analysis_report_summary.csv\n",
      "\n",
      "Analysis Summary:\n",
      "Total Sites Analyzed: 21\n",
      "Sites with Faults: 0\n",
      "Total Fault Periods: 0\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    # Parameters\n",
    "    CSV_FILE_PATH = 'PE(Sheet1).csv'  \n",
    "    START_DATE = '2025-07-07'\n",
    "    END_DATE = '2025-07-09'\n",
    "    OUTPUT_FILE = 'inverter_fault_analysis_report.csv'\n",
    "    GRID_RESOLUTION = '5min'\n",
    "    \n",
    "    print(\"Starting Multi-Site Inverter Module Fault Analysis...\")\n",
    "    print(f\"Date Range: {START_DATE} to {END_DATE}\")\n",
    "    print(f\"Grid Resolution: {GRID_RESOLUTION}\")\n",
    "    print(f\"Output File: {OUTPUT_FILE}\")\n",
    "    \n",
    "    # Run the analysis\n",
    "    fault_df, summary_df = run_multi_site_analysis(\n",
    "        csv_file_path=CSV_FILE_PATH,\n",
    "        start_date=START_DATE,\n",
    "        end_date=END_DATE,\n",
    "        output_file=OUTPUT_FILE,\n",
    "        grid=GRID_RESOLUTION\n",
    "    )\n",
    "    \n",
    "    return fault_df, summary_df \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results_df, summary_df = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3787a651-e296-4260-aa16-76491ed54d39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
